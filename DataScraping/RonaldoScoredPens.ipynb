{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dd9e7368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Create request and get the data from the websites\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'}\n",
    "\n",
    "#Looking on the TransferMarket website, there are 10 pages related to ronaldo's scored penalties\n",
    "noPages = 10\n",
    "pageBaseURL = \"https://www.transfermarkt.com/cristiano-ronaldo/elfmetertore/spieler/8198/ajax/yw1/saison_id//wettbewerb_id//plus/1/page/\"\n",
    "pages = [pageBaseURL + str(i+1) for i in range(noPages)]\n",
    "#pages is now an array of URLs referencing all the pages we want access to. pageBaseURL1 pageBaseURL2 pageBaseURL3 and so on.\n",
    "\n",
    "#pageSoups is all the HTML data we need to scrape for each page\n",
    "pageSoups = np.zeros(noPages, dtype=object)\n",
    "for no in range(noPages):\n",
    "    page = pages[no]\n",
    "    pageTree = requests.get(page, headers=headers)\n",
    "    pageSoups[no] = (BeautifulSoup(pageTree.content, 'html.parser'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "71b6fcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Data from the soup\n",
    "Season = []\n",
    "Competition = []\n",
    "Club = []\n",
    "Date = []\n",
    "HomeTeam = []\n",
    "Final_result = []\n",
    "AwayTeam = []\n",
    "Minute = []\n",
    "ScoreAfterPK = []\n",
    "Goalkeeper = []\n",
    "\n",
    "allData = [Season,\n",
    "    Competition,\n",
    "    Club,\n",
    "    Date,\n",
    "    HomeTeam,\n",
    "    Final_result,\n",
    "    AwayTeam,\n",
    "    Minute,\n",
    "    ScoreAfterPK,\n",
    "    Goalkeeper]\n",
    "\n",
    "\n",
    "#First we need to refine the HTML to the tables we want\n",
    "tableSoups = []\n",
    "for pageSoup in pageSoups:\n",
    "    tableSoups.append(pageSoup.find_all('table', 'items'))\n",
    "\n",
    "#now get the data from the table\n",
    "for table in tableSoups:\n",
    "    #The table variable contains a list. You need to call .tbody on its [0] member, not on the entire thing.\n",
    "    body = table[0].tbody\n",
    "    \n",
    "    #We need to go into the tr rows and get the td column data for each row\n",
    "    #get a list of the rows\n",
    "    rows = body.contents\n",
    "    \n",
    "    for row in rows:\n",
    "        #some entries are '\\n'. Skip them\n",
    "        if row == '\\n':\n",
    "            continue\n",
    "        \n",
    "        #now get the column data from the rows\n",
    "        columns = row.contents\n",
    "        \n",
    "        arrayNo = 0\n",
    "        for column in columns:\n",
    "            #some entries are '\\n'. Skip them\n",
    "            if column == '\\n':\n",
    "                continue\n",
    "                \n",
    "            allData[arrayNo].append(column)\n",
    "            arrayNo += 1\n",
    "        \n",
    "        if arrayNo != 10:\n",
    "            raise ValueError(\"Not all arrays have been updated\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "28827d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Now we need to refine the data so that we have strings/links\n",
    "\n",
    "#Usually there are two ways to refine\n",
    "\n",
    "def strRefine(data):\n",
    "    temp = []\n",
    "    for d in data:\n",
    "        #.string is the attribute for the contents of s\n",
    "        temp.append(d.string)\n",
    "    return temp\n",
    "\n",
    "def imgRefine(data):\n",
    "    temp = []\n",
    "    for d in data:\n",
    "        #Change the entry into a string, and then split that string to get between\n",
    "        #\" class and <img alt=\"\n",
    "        temp.append(str(d).split('<img alt=\"',1)[1].split('\" class',1)[0])\n",
    "    return temp\n",
    "\n",
    "Season = strRefine(Season)\n",
    "Competition = imgRefine(Competition)\n",
    "Club = imgRefine(Club)\n",
    "Date = strRefine(Date)\n",
    "HomeTeam = imgRefine(HomeTeam)\n",
    "Final_result = strRefine(Final_result)\n",
    "AwayTeam = imgRefine(AwayTeam)\n",
    "Minute = strRefine(Minute)\n",
    "ScoreAfterPK = strRefine(ScoreAfterPK)\n",
    "Goalkeeper = strRefine(Goalkeeper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3599a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put all into a Pandas array and save as a csv\n",
    "d = {'Season': Season,\n",
    "    'Competition': Competition,\n",
    "    'Club': Club,\n",
    "    'Date': Date,\n",
    "    'HomeTeam': HomeTeam,\n",
    "    'Final_result': Final_result,\n",
    "    'AwayTeam': AwayTeam,\n",
    "    'Minute': Minute,\n",
    "    'ScoreAfterPK': ScoreAfterPK,\n",
    "    'Goalkeeper': Goalkeeper}\n",
    "df = pd.DataFrame(d)\n",
    "df.to_csv(\"Ronaldo Penalties Scored.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
